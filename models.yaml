# Model configuration file
# Contains all model lists used in the GenAI Playground app

sentence_transformer_encoders:
  - "all-MiniLM-L6-v2 (~22M)"
  - "all-mpnet-base-v2 (~110M)"
  - "paraphrase-MiniLM-L6-v2 (~22M)"
  - "sentence-transformers/all-MiniLM-L12-v2 (~33M)"
  - "sentence-transformers/all-distilroberta-v1 (~82M)"
  - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (~33M)"

other_encoders:
  - "distilbert-base-uncased (~66M)"
  - "bert-base-uncased (~110M)"
  - "roberta-base (~125M)"

other_decoder_models:
  - "gpt2 (~124M)"
  - "gpt2-medium (~355M)"
  - "gpt2-large (~774M)"
  - "gpt2-xl (~1.5B)"
  - "openai-community/gpt2-large (~774M)"
  - "tiiuae/falcon-rw-1b (~1B)"
  - "tiiuae/falcon-7b (~7B)"
  - "mistralai/Mistral-7B-v0.1 (~7B)"
  - "mistralai/Mistral-7B-Instruct-v0.2 (~7B)"
  - "meta-llama/Llama-2-7b-hf (~7B)"
  - "meta-llama/Llama-2-13b-hf (~13B)"
  - "meta-llama/Llama-2-70b-hf (~70B)"
  - "Qwen/Qwen-7B-Chat (~7B)"
  - "google/gemma-7b (~7B)"
  - "EleutherAI/gpt-j-6B (~6B)"
  - "EleutherAI/gpt-neo-1.3B (~1.3B)"
  - "EleutherAI/gpt-neo-2.7B (~2.7B)"
  # Quantized models
  - "TheBloke/Llama-2-7B-GPTQ (Quantized, GPTQ)"
  - "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ (Quantized, GPTQ)"

qa_tuned_models:
  # Only encoder-decoder models compatible with text2text-generation pipeline
  - "google/flan-t5-base (~250M)"
  - "google/flan-t5-small (~80M)"
  - "t5-small (~60M)"
  - "t5-base (~220M)"
  - "facebook/bart-base (~140M)"

other_encoder_decoder_models:
  - "t5-small (~60M)"
  - "t5-base (~220M)"
  - "t5-large (~770M)"
  - "facebook/bart-base (~140M)"
  - "facebook/bart-large-cnn (~400M)"
  - "google/pegasus-xsum (~568M)"
  - "google/mt5-small (~300M)"
  - "google/mt5-base (~580M)"
  - "google/mt5-large (~1.2B)"
  - "facebook/mbart-large-50-many-to-many-mmt (~610M)"
  - "google/flan-t5-large (~780M)" 